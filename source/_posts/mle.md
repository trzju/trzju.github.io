---
title: 最大似然估计 (MLE) 与加权最大似然估计 (WMLE)
date: 2026-1-16
categories: [Math in RL]
tags: [RL, Math, Notes]
---


## 最大似然估计 (MLE) 与加权最大似然估计 (WMLE)

#### 1. 数学原理：从“硬币抛掷”说起

**最大似然估计 (MLE, Maximum Likelihood Estimation)**

- **直觉**：事情已经发生了，反推“什么样的参数最可能导致这件事发生”。
- **例子**：抛硬币 10 次，7 次正面，3 次反面。
  - 如果硬币是均匀的 ($P=0.5$)，发生这种情况的概率比较小。
  - 如果硬币是不均匀的 ($P=0.7$)，发生这种情况的概率最大。
  - MLE 表明：既然看到了 7 次正面，那么 $P=0.7$ 就是当前数据的**最大似然估计**。

**数学定义**：

给定独立同分布 (i.i.d.) 数据集 $D = \{x_1, x_2, ..., x_N\}$，模型参数为 $\theta$。

目标是最大化似然函数 $P(D|\theta)$。通常取对数（Log-Likelihood）变乘积为求和，方便计算梯度：

$$
\theta_{MLE}^* = \arg \max_\theta \sum_{i=1}^N \log P_\theta(x_i)
$$

------

**加权最大似然估计 (WMLE, Weighted MLE)**

- **直觉**：有些数据比其他数据“更重要”，或者我们希望模型“更重视”某些样本。
- **例子**：还是抛硬币，但第 3 次抛掷（正面）是在“决定胜负的关键时刻”抛的，我们希望模型能优先拟合这一关键数据。我们给它 10 倍的权重。

**数学定义**：

给每个样本 $x_i$ 引入一个权重 $w_i$：

$$
\theta_{WMLE}^* = \arg \max_\theta \sum_{i=1}^N w_i \log P_\theta(x_i)
$$

------

#### 2. 深度学习中的应用：修正与辅助

在传统的监督学习（Supervised Learning）中，MLE 是绝对的主角，而 WMLE 通常作为一种“补丁”或“修正手段”出现。

| **方法** | **应用场景**                       | **解释**                                                     |
| -------- | ---------------------------------- | ------------------------------------------------------------ |
| **MLE**  | **标准损失函数**                   | **交叉熵损失 (Cross Entropy)** 本质上就是负的 Log-Likelihood。我们在训练猫狗分类器时，就是在最大化“给定图片 $x$，模型预测正确标签 $y$ 的概率 $\log P(y)$ |
| **WMLE** | **类别不平衡 (Class Imbalance)**   | 如果数据集中有 1000 张狗，只有 10 张猫。模型只要全猜“狗”就能达到 99% 准确率。为了强迫模型学到猫的特征，我们给猫的样本赋予极大的权重 $w_{cat} \gg w_{dog}$（例如 **Focal Loss**）。 |
| **WMLE** | **课程学习 (Curriculum Learning)** | 在训练初期，给简单的样本高权重；训练后期，给难样本高权重。   |

在 DL 中，我们通常认为**数据标签是真理** (Ground Truth)。MLE 负责逼近这个真理，WMLE 负责解决数据分布不均的问题。

------

#### 3. 强化学习 (RL) 中的应用：核心驱动力

在强化学习中，WMLE **不再是“补丁”，而是策略优化算法（如 Policy Gradient）的灵魂。**

结合 CS285 内容来理解。

**1. 模仿学习 (Behavioral Cloning) = 标准 MLE** (这也是为什么CS285从模仿学习开始，因为这很类似于监督学习)

- 如果我们有专家的轨迹 $\tau_{expert}$，我们希望策略 $\pi_\theta$ 模仿专家。
- 这就是标准的监督学习。我们最大化 $\sum \log \pi_\theta(a_{expert}|s)$。
- **含义**：“专家这么做是对的，我要尽可能提高‘在这个状态做这个动作’的概率。”

**2. 策略梯度 (Policy Gradient) = 加权 MLE**

- 在 RL 中，我们没有“正确标签”。动作好不好，取决于未来的回报。

- 回顾 Policy Gradient 的核心公式 ：

  $$
  \nabla J(\theta) = E_{\tau} \left[ \sum_t \nabla \log \pi_\theta(a_t|s_t) \cdot \underbrace{A(s_t, a_t)}_{\text{Weight } w} \right]
  $$

  这个公式的梯度，等价于我们在做 WMLE，其中**权重 $w$ 就是优势函数 (Advantage) $A_t$**。
  
  - **当 $A_t > 0$ (好动作)**：权重为正。WMLE 就像模仿学习一样，**提高**该动作的概率。
  - **当 $A_t < 0$ (坏动作)**：权重为负。WMLE 会**降低**该动作的概率。
  - **当 $A_t \approx 0$ (普通动作)**：权重接近 0。模型忽略该样本，不更新参数。

**3. 离策略修正 (Off-Policy Correction) = 重要性采样权重**

- 在 Off-Policy Actor-Critic 中 ，我们使用行为策略 $\beta$ 产生的数据来更新目标策略 $\pi$。

- 此时的权重 $w$ 变成了 **重要性采样比率 (Importance Sampling Ratio)**：

  $$
  w_t = \frac{\pi_\theta(a_t|s_t)}{\beta(a_t|s_t)} \cdot A_t
  $$

  这里有两个层面的加权：

  1. **纠偏权重** $\frac{\pi}{\beta}$：因为数据来源不同，必须加权修正分布（纯统计学意义的 WMLE）。
  2. **强化权重** $A_t$：因为我们要最大化奖励，必须根据好坏加权（RL 意义的 WMLE）。

------

#### 4. 核心对比总结表

| **维度**            | **深度学习 (Deep Learning)**                         | **强化学习 (Reinforcement Learning)**                        |
| ------------------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| **数据性质**        | 静态、有标签 (Static, Labeled)                       | 动态、无标签、带反馈 (Dynamic, Reward)                       |
| **MLE 的角色**      | **主宰**。直接拟合数据的分布。                       | **模仿者**。仅用于模仿学习 (Behavioral Cloning)，假设数据是完美的。几乎把强化学习问题当作一个监督学习问题。 |
| **WMLE 的角色**     | **修补匠**。用于处理长尾分布、样本不平衡或噪声数据。 | **领航员**。**这是 RL 的核心机制**。通过将回报 (Return/Advantage) 作为权重，WMLE 将“拟合数据”变成了“试错学习”——好的多拟合，坏的少拟合（甚至反向拟合）。 |
| **权重 $w$ 的来源** | 人为定义的（如类别频率的倒数）。                     | 环境反馈的（奖励函数）或 策略间的差异（重要性采样）。        |

**In a word: **

在深度学习中，我们用 MLE 学习“什么是对的”；在强化学习中，我们用加权 MLE (WMLE) 探索“什么更好”。
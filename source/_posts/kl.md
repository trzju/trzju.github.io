---
title: KL 散度与自然梯度
date: 2026-1-16
categories: [Math in RL]
tags: [RL, Math, Notes]
---


## KL 散度与自然梯度

**不仅要“走对方向”（梯度），还要“走对步长”（几何结构）。**

这就是 **KL 散度 (KL Divergence)** 和 **自然梯度 (Natural Gradient)** 解决的核心问题。

#### 1. why？

在普通的梯度下降（Gradient Descent, GD）中，我们更新参数 $\theta \leftarrow \theta - \alpha \nabla J$。

这里有一个巨大的**隐患**：我们假设**参数空间 (Parameter Space)** 的距离等同于**概率分布空间 (Distribution Space)** 的距离。

**直觉反例**：

想象你在调节一个神经网络的两个参数：

- **场景 A**：参数 $\theta$ 变了 0.1，输出的策略概率分布从 `[0.5, 0.5]` 变成了 `[0.51, 0.49]`。这是微小的变化。
- **场景 B**：参数 $\theta$ 变了 0.1，输出的策略概率分布从 `[0.99, 0.01]` 变成了 `[0.01, 0.99]`。这是剧变！

在欧几里得空间（Euclidean Space）里，这两次更新的“步长”都是 $\|\Delta \theta\| = 0.1$。但在功能的意义上，场景 B 是灾难性的（策略完全反转了）。

**普通梯度下降看不出这种区别**，它可能会在场景 B 中迈出太大的一步，导致策略崩溃。

我们需要一把新的尺子，不是量参数变了多少，而是量**分布变了多少**。这把尺子就是 **KL 散度**。

------

#### 2. 数学基础：KL 散度 (Kullback-Leibler Divergence)

KL 散度用于衡量两个概率分布 $P$ 和 $Q$ 之间的差异。

**定义**：

$$
D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} = E_{x \sim P} [\log P(x) - \log Q(x)]
$$

**数学实例 1：简单的离散分布**

假设有三个骰子分布：

- **P (基准)**: `[0.4, 0.6]` (偏向反面)
- **Q (分布一)**: `[0.5, 0.5]` (均匀)
- **R (分布二)**: `[0.1, 0.9]` (极度偏向反面)

我们来看 P 变成 Q 和 P 变成 R 的差异：

1. **P vs Q**:

   $$
   \begin{aligned} D_{KL}(P||Q) &= 0.4 \log(\frac{0.4}{0.5}) + 0.6 \log(\frac{0.6}{0.5}) \\ &\approx 0.4(-0.223) + 0.6(0.182) \approx \mathbf{0.02} \end{aligned}
   $$

   *差异很小。*

2. **P vs R**:

   $$
   \begin{aligned} D_{KL}(P||R) &= 0.4 \log(\frac{0.4}{0.1}) + 0.6 \log(\frac{0.6}{0.9}) \\ &\approx 0.4(1.386) + 0.6(-0.405) \approx \mathbf{0.31} \end{aligned}
   $$

   *差异很大（大约是前者的 15 倍）。*

**关键性质**：

1. **非负性**：$D_{KL} \ge 0$。当且仅当 $P=Q$ 时为 0。
2. **非对称性**：$D_{KL}(P||Q) \neq D_{KL}(Q||P)$。所以它不是严格意义上的“距离”，但在局部（微小变化时）是对称的。

------

#### 3. 核心痛点：欧几里得梯度的“盲目”

为了引出自然梯度，我们需要证明标准梯度下降在某些情况下是很笨的。

**数学实例 2：高斯分布的参数移动**

假设我们的模型是一个高斯分布 $\mathcal{N}(\mu, \sigma^2)$，参数是均值 $\mu$。

我们要把 $\mu$ 移动 $\Delta \mu = 1$。这代表参数空间的距离是 1。

- **情况 A：大方差 ($\sigma = 10$)**

  从 $\mathcal{N}(0, 100)$ 移到 $\mathcal{N}(1, 100)$。

  这两个大胖子分布几乎完全重叠。

  $D_{KL} \approx 0.005$ (微乎其微)。

  *在这里，参数动 1，分布几乎没变。*

- **情况 B：小方差 ($\sigma = 0.1$)**

  从 $\mathcal{N}(0, 0.01)$ 移到 $\mathcal{N}(1, 0.01)$。

  这两个瘦高个分布几乎没有重叠（一个在 0 附近，一个在 1 附近，且都很窄）。

  $D_{KL} \approx 50$ (巨大差异)。

  *在这里，参数动 1，分布已经天翻地覆。*

**结论**：同样的参数更新步长 $\Delta \theta$，在不同的“地形”（不同的 $\sigma$）下，代表的实际意义（分布变化）完全不同。

普通梯度下降不管 $\sigma$ 是多少，只管 $\mu$ 的导数。**这就是所谓的“参数空间与分布空间的不匹配”。**

------

#### 4. 解决方案：自然梯度 (Natural Gradient)

我们希望更新步长在**分布空间**中是恒定的，而不是在参数空间中。

即：我们希望限制 $D_{KL}(\pi_\theta || \pi_{\theta + \Delta \theta}) = \epsilon$（常数），而不是 $\|\Delta \theta\|^2 = \epsilon$。

**费雪信息矩阵 (Fisher Information Matrix, FIM)**

为了实现这一点，我们需要知道参数空间的局部“曲率”。FIM ($F$) 就是描述这个曲率的矩阵。

它近似了 KL 散度的二阶泰勒展开（Hessian）：

$$
D_{KL}(\pi_\theta || \pi_{\theta + \Delta \theta}) \approx \frac{1}{2} \Delta \theta^T F \Delta \theta
$$

其中 $F$ 定义为梯度的协方差矩阵：

$$
F = E_{x \sim \pi} [\nabla_\theta \log \pi(x|\theta) \nabla_\theta \log \pi(x|\theta)^T]
$$

**自然梯度公式**

普通的梯度 $\nabla J$ 指向的是参数变化最快使得 Loss 下降的方向（欧几里得方向）。

而**自然梯度 $\tilde{\nabla} J$** 指向的是分布变化最快使得 Loss 下降的方向（黎曼流形方向）。

两者通过 $F$ 联系：

$$
\tilde{\nabla} J = F^{-1} \nabla J
$$

**直觉理解公式**：

- $F$ 很大 $\rightarrow$ 曲率大（如情况 B，方差小，稍微一动分布就剧变） $\rightarrow$ $F^{-1}$ 很小 $\rightarrow$ **自然梯度会自动把更新步长缩小**，防止走过头。
- $F$ 很小 $\rightarrow$ 曲率平坦（如情况 A，方差大，怎么动分布都不变） $\rightarrow$ $F^{-1}$ 很大 $\rightarrow$ **自然梯度会自动放大步长**，加快学习速度。

------

#### 5. 在强化学习中的应用 (TRPO & PPO)

在 RL 中，自然梯度的思想至关重要，因为策略网络的分布如果变动太大，Agent 可能会采集到极其糟糕的数据，导致“策略坍塌”且无法恢复。

**1. TRPO (Trust Region Policy Optimization)**

TRPO 是自然梯度的直接应用。它试图求解：

$$
\max_\theta E[\dots] \quad \text{s.t.} \quad D_{KL}(\pi_{old} || \pi_{new}) \le \delta
$$

这个约束优化问题的解析解这就指向了自然梯度方向：$\Delta \theta \propto F^{-1} \nabla J$。

*工程挑战*：矩阵 $F$ 可能有几百万维（参数量），求逆 $F^{-1}$ 计算量太大。TRPO 使用“共轭梯度法”来近似求解 $F^{-1}x$。

**2. PPO (Proximal Policy Optimization)**

PPO 觉得计算自然梯度还是太麻烦了。它用更暴力的手段来“模拟”自然梯度的效果：

- 如果不计算 $F^{-1}$，我直接把概率的比值 $r(\theta) = \frac{\pi_{new}}{\pi_{old}}$ 截断 (Clip) 在 $[0.8, 1.2]$ 之间。
- 这样，虽然我没有显式地计算 KL 散度和曲率，但我强制保证了 $\pi_{new}$ 不会偏离 $\pi_{old}$ 太远。

#### 总结

1. **距离的错觉**：参数距离 $\neq$ 分布距离。参数变一点点，分布可能变很多（尤其是在确定性高或方差小的时候）。
2. **KL 散度**：是衡量**分布距离**的正确尺子。
3. **费雪信息矩阵 (FIM)**：是参数空间到分布空间的“汇率换算表”（度量张量）。它告诉你在当前位置，参数动一下，分布动多少。
4. **自然梯度**：利用 FIM 对普通梯度进行校正（$F^{-1} \nabla J$）。它能根据地形自动调节步长——平坦时大步走，陡峭时小步走。
5. **RL 意义**：保证策略更新的稳定性，是 TRPO 和 PPO 算法背后的数学灵魂。